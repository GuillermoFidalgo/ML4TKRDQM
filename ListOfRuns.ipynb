{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import __future__\n",
    "import codecs, os, re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from: https://cmsweb.cern.ch/dqm/offline/data/browse/ROOT/OfflineData/\n",
      "list of relative urls:  ['Run2018/ZeroBias/', 'Run2017/ZeroBias/', 'Run2016/ZeroBias/']\n"
     ]
    }
   ],
   "source": [
    "baseurl='https://cmsweb.cern.ch/dqm/offline/data/browse/ROOT/OfflineData/'\n",
    "url_runs_2018=\"Run2018/ZeroBias/\"\n",
    "url_runs_2017=\"Run2017/ZeroBias/\"\n",
    "url_runs_2016=\"Run2016/ZeroBias/\"\n",
    "# url_runs_2015=\"Run2015/ZeroBias/\"\n",
    "\n",
    "\n",
    "url_runs=[url_runs_2018,url_runs_2017,url_runs_2016]#,url_runs_2015]\n",
    "print \"Fetching from: \"+baseurl\n",
    "print \"list of relative urls: \",url_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section shows the procedure to parse one instance and should be adapted in a for loop for all runs and all years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create dump the html page into a document using `curl` command and specifying the path to the certificate.\n",
    "In this example we are looking at runs from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url_runs_2018+\" > index.html\") \n",
    "# This dumps the html page into a file which I called \"index.html\" \n",
    "#Check the directory you are working in to see if it's there\n",
    "\n",
    "f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "index = f.readlines()                  # read the document\n",
    "a=str(index)                           # convert to string\n",
    "soup = BeautifulSoup(a, 'html.parser') # create the soup object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e3fd4dc60664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
     ]
    }
   ],
   "source": [
    "print soup.title.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/dqm/offline/data/browse/ROOT/OfflineData/Run2017\">Up</a>\n"
     ]
    }
   ],
   "source": [
    "print soup.a # This will show the first instance of and anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One way to parse the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7983eb1aabd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We want to get all the anchor tags <a>...<\\a> from the body of the HTML page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"tag 10 is \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# We want to get all the anchor tags <a>...<\\a> from the body of the HTML page \n",
    "tag = soup.body.find_all(\"a\")\n",
    "\n",
    "print \"tag 10 is \"+tag[10].string\n",
    "\n",
    "RUNSXX=[]\n",
    "i=1\n",
    "for child in tag:\n",
    "    if child.string != 'Up':\n",
    "        print \"%s)\"%i,child.string  \n",
    "        i+=1\n",
    "        RUNSXX.append(child.string)\n",
    "        \n",
    "print \"RUNSXX has this many items:\",len(RUNSXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way of finding the runs using Regular Expressions\n",
    "\n",
    "Since we know that the list of runs all share the first `000` we can use Regular expressions to create a list much like before with one single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cadee3ad0845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRUNSXXRE\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"000.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# the \".\" for a regular expresions means that it will expect ANY character EXCEPT newlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print len(RUNSXXRE),\"items\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mRUNSXXRE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "RUNSXXRE= soup.body.find_all(string=re.compile(\"000.\")) \n",
    "# the \".\" for a regular expresions means that it will expect ANY character EXCEPT newlines\n",
    "\n",
    "# print len(RUNSXXRE),\"items\"\n",
    "print RUNSXXRE\n",
    "if len(RUNSXXRE)==0:\n",
    "    print \"List is empty\"\n",
    "else:\n",
    "    print \"List has\",len(RUNSXXRE),\"items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Time to parse and collect all run numbers for one specific year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a9acb9aff5bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# convert to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create the soup object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mRUNSXXRE\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"000.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrundir_with_rootfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url_runs_2018+\" > index.html\") \n",
    "# This dumps the html page into a file which I called \"index.html\" \n",
    "#Check the directory you are working in to see if it's there\n",
    "\n",
    "f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "index = f.readlines()                  # read the document\n",
    "a=str(index)                           # convert to string\n",
    "soup = BeautifulSoup(a, 'html.parser') # create the soup object\n",
    "RUNSXXRE= soup.body.find_all(string=re.compile(\"000.\")) \n",
    "\n",
    "rundir_with_rootfiles=[]\n",
    "runs_with_rootfiles=[]\n",
    "rundir_without_rootfiles=[]\n",
    "# runs_without_rootfiles=[]\n",
    "\n",
    "\n",
    "for rundir in RUNSXXRE:\n",
    "    os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url_runs_2018+rundir+\" > index.html\") \n",
    "    f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "    index = f.readlines()                  # read the document\n",
    "    a=str(index)                           # convert to string\n",
    "    soup = BeautifulSoup(a, 'html.parser') # create the soup object\n",
    "    entries= soup.body.find_all(string=re.compile(\"DQM.\"))\n",
    "    if len(entries)==0:\n",
    "        rundir_without_rootfiles.append(str(rundir)) # Fill the list\n",
    "        #print runs,\"is empty\"\n",
    "    else:\n",
    "        \n",
    "        for n in re.findall(r\"R(\\d+)\",str(entries)): # This will only keep the 6 digits of the run numbers that we need (without the \"R000\")\n",
    "            x=str(int(n))                            # Since we wont want to do math with these numbers I will convert them to strings\n",
    "            runs_with_rootfiles.append(x)            # Fill the list\n",
    "        rundir_with_rootfiles.append(str(rundir))         # Fill the list\n",
    "        \n",
    "        #print runs,\"has\",len(entries),\"root files\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "For Run2018/ZeroBias/\n",
      "Out of a total of"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RUNSXXRE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e47dbbfdd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"===========================================================\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"For\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murl_runs_2018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Out of a total of\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUNSXXRE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"run directories:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrundir_with_rootfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"have root files\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrundir_without_rootfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"are empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"There are\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns_with_rootfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"runs with root files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print \"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUNSXXRE' is not defined"
     ]
    }
   ],
   "source": [
    "print \"===========================================================\"\n",
    "print \"For\",url_runs_2018\n",
    "print \"Out of a total of\",len(RUNSXXRE),\"run directories:\\n\",len(rundir_with_rootfiles),\"have root files\\n\",len(rundir_without_rootfiles),\"are empty\"\n",
    "print \"There are\",len(runs_with_rootfiles),\"runs with root files\"\n",
    "# print \"\\n\"\n",
    "# print \"List of available runs for this year\\n\\n\",runs_with_rootfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is for all runs and all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from: https://cmsweb.cern.ch/dqm/offline/data/browse/ROOT/OfflineData/\n",
      "list of relative urls:  ['Run2018/ZeroBias/', 'Run2017/ZeroBias/', 'Run2016/ZeroBias/']\n",
      "===========================================================\n",
      "For Run2018/ZeroBias/\n",
      "Out of a total of 106 run directories:\n",
      "73 have root files\n",
      "33 are empty\n",
      "There are 1185 runs with root files\n",
      "\n",
      "\n",
      "===========================================================\n",
      "For Run2017/ZeroBias/\n",
      "Out of a total of 102 run directories:\n",
      "27 have root files\n",
      "75 are empty\n",
      "There are 286 runs with root files\n",
      "\n",
      "\n",
      "===========================================================\n",
      "For Run2016/ZeroBias/\n",
      "Out of a total of 104 run directories:\n",
      "0 have root files\n",
      "104 are empty\n",
      "There are 0 runs with root files\n",
      "\n",
      "\n",
      "===========================================================\n",
      "For GLOBAL\n",
      "Out of all run directories:\n",
      "100 have root files\n",
      "212 are empty\n",
      "There are 1471 runs with root files\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseurl='https://cmsweb.cern.ch/dqm/offline/data/browse/ROOT/OfflineData/'\n",
    "url_runs_2018=\"Run2018/ZeroBias/\"\n",
    "url_runs_2017=\"Run2017/ZeroBias/\"\n",
    "url_runs_2016=\"Run2016/ZeroBias/\"\n",
    "# url_runs_2015=\"Run2015/ZeroBias/\"\n",
    "\n",
    "\n",
    "url_runs=[url_runs_2018,url_runs_2017,url_runs_2016]#,url_runs_2015]\n",
    "print \"Fetching from: \"+baseurl\n",
    "print \"list of relative urls: \",url_runs\n",
    "\n",
    "global_rundir_with_rootfiles=[]\n",
    "global_runs_with_rootfiles=[]\n",
    "global_rundir_without_rootfiles=[]\n",
    "\n",
    "# runs_without_rootfiles=[]\n",
    "for url in url_runs:\n",
    "    os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url+\" > index.html\") \n",
    "    f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "    index = f.readlines()                  # read the document\n",
    "    a=str(index)                           # convert to string\n",
    "    soup = BeautifulSoup(a, 'html.parser') # create the soup object\n",
    "    RUNSXXRE= soup.body.find_all(string=re.compile(\"000.\")) # the \".\" for a regular expresions means that it will expect ANY character EXCEPT newlines\n",
    "  \n",
    "    rundir_with_rootfiles=[]\n",
    "    runs_with_rootfiles=[]\n",
    "    rundir_without_rootfiles=[]\n",
    "    \n",
    "    \n",
    "    for rundir in RUNSXXRE:\n",
    "        os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url+rundir+\" > index.html\") \n",
    "        f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "        index = f.readlines()                  # read the document\n",
    "        a=str(index)                           # convert to string\n",
    "        soup = BeautifulSoup(a, 'html.parser') # create the soup object\n",
    "        entries= soup.body.find_all(string=re.compile(\"DQM.\"))   \n",
    "        if len(entries)==0:\n",
    "            rundir_without_rootfiles.append(str(rundir))        # Fill the list\n",
    "            global_rundir_without_rootfiles.append(str(rundir)) # Fill the list\n",
    "            #print rundir,\"is empty\"\n",
    "        else:\n",
    "\n",
    "            for n in re.findall(r\"R(\\d+)\",str(entries)):    # This will only keep the 6 digits of the run numbers that we need (without the \"R000\")\n",
    "                x=str(int(n))                               # Since we wont want to do math with these numbers I will convert them to strings\n",
    "                runs_with_rootfiles.append(x)               # Fill the list\n",
    "                global_runs_with_rootfiles.append(x)        # Fill the list\n",
    "                \n",
    "            rundir_with_rootfiles.append(str(rundir))         # Fill the list\n",
    "            global_rundir_with_rootfiles.append(str(rundir))  # Fill the list\n",
    "\n",
    "            #print rundir,\"has\",len(entries),\"root files\"    \n",
    "    print \"===========================================================\"\n",
    "    print \"For\",url\n",
    "    print \"Out of a total of\",len(RUNSXXRE),\"run directories:\\n\",len(rundir_with_rootfiles),\"have root files\\n\",len(rundir_without_rootfiles),\"are empty\"\n",
    "    print \"There are\",len(runs_with_rootfiles),\"runs with root files\"\n",
    "    print \"\\n\"\n",
    "    #print \"List of available runs for this year\\n\\n\",runs_with_rootfiles\n",
    "    \n",
    "print \"===========================================================\"\n",
    "print \"For GLOBAL\"\n",
    "print \"Out of all run directories:\\n\",len(global_rundir_with_rootfiles),\"have root files\\n\",len(global_rundir_without_rootfiles),\"are empty\"\n",
    "print \"There are\",len(global_runs_with_rootfiles),\"runs with root files\"\n",
    "print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to see the diferent list of runs just do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0003256xx/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For runs WITH root files\n",
    "global_rundir_with_rootfiles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catching the repeated instances of the run numbers due to variations of the root file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "repeatruns=[]\n",
    "for run in global_runs_with_rootfiles: \n",
    "    if run == global_runs_with_rootfiles[-1]:\n",
    "#         print \"Last run\",run\n",
    "        break\n",
    "    if run==global_runs_with_rootfiles[i+1]:\n",
    "        #print \"repeated run:\",run\n",
    "        repeatruns.append(run)\n",
    "    i+=1\n",
    "#print 'there are',len(repeatruns),'repeated runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['319910',\n",
       " '318877',\n",
       " '317182',\n",
       " '316218',\n",
       " '306459',\n",
       " '306459',\n",
       " '306459',\n",
       " '306459',\n",
       " '306459',\n",
       " '306459',\n",
       " '306456',\n",
       " '306456',\n",
       " '306456',\n",
       " '306456',\n",
       " '306456',\n",
       " '306432',\n",
       " '306422',\n",
       " '306417',\n",
       " '306171',\n",
       " '306170',\n",
       " '306169',\n",
       " '306155',\n",
       " '306154',\n",
       " '306139',\n",
       " '306137',\n",
       " '306135',\n",
       " '306126',\n",
       " '306125',\n",
       " '306125',\n",
       " '306095',\n",
       " '306093',\n",
       " '306091',\n",
       " '306051',\n",
       " '306042',\n",
       " '306041',\n",
       " '305902',\n",
       " '305842',\n",
       " '305841',\n",
       " '305832',\n",
       " '305821',\n",
       " '305636',\n",
       " '305590',\n",
       " '305588',\n",
       " '305586',\n",
       " '305518',\n",
       " '305517',\n",
       " '305406',\n",
       " '305365',\n",
       " '305350',\n",
       " '305341',\n",
       " '305338',\n",
       " '305312',\n",
       " '305311',\n",
       " '305310',\n",
       " '305252',\n",
       " '305248',\n",
       " '305247',\n",
       " '305237',\n",
       " '305236',\n",
       " '305202',\n",
       " '305188',\n",
       " '305186',\n",
       " '305185',\n",
       " '305184',\n",
       " '305182',\n",
       " '305181',\n",
       " '305180',\n",
       " '305178',\n",
       " '305114',\n",
       " '305112',\n",
       " '305081',\n",
       " '305064',\n",
       " '305064',\n",
       " '305064',\n",
       " '305064',\n",
       " '305064',\n",
       " '305064',\n",
       " '305062',\n",
       " '305059',\n",
       " '305046',\n",
       " '305040',\n",
       " '299481',\n",
       " '299481',\n",
       " '299481',\n",
       " '299480',\n",
       " '299480',\n",
       " '299480',\n",
       " '299479',\n",
       " '299479',\n",
       " '299479',\n",
       " '299478',\n",
       " '299478',\n",
       " '299478',\n",
       " '299477',\n",
       " '299477',\n",
       " '299477']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeatruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0003218xx/',\n",
       " '0003217xx/',\n",
       " '0003216xx/',\n",
       " '0003215xx/',\n",
       " '0003214xx/',\n",
       " '0003213xx/',\n",
       " '0003212xx/',\n",
       " '0003211xx/',\n",
       " '0003210xx/',\n",
       " '0003209xx/',\n",
       " '0003208xx/',\n",
       " '0003207xx/',\n",
       " '0003206xx/',\n",
       " '0003205xx/',\n",
       " '0003204xx/',\n",
       " '0003203xx/',\n",
       " '0003202xx/',\n",
       " '0003201xx/',\n",
       " '0003192xx/',\n",
       " '0003191xx/',\n",
       " '0003186xx/',\n",
       " '0003185xx/',\n",
       " '0003184xx/',\n",
       " '0003183xx/',\n",
       " '0003182xx/',\n",
       " '0003181xx/',\n",
       " '0003180xx/',\n",
       " '0003179xx/',\n",
       " '0003178xx/',\n",
       " '0003177xx/',\n",
       " '0003074xx/',\n",
       " '0003073xx/',\n",
       " '0003072xx/',\n",
       " '0003071xx/',\n",
       " '0003070xx/',\n",
       " '0003069xx/',\n",
       " '0003068xx/',\n",
       " '0003067xx/',\n",
       " '0003066xx/',\n",
       " '0003065xx/',\n",
       " '0003063xx/',\n",
       " '0003062xx/',\n",
       " '0003048xx/',\n",
       " '0003047xx/',\n",
       " '0003046xx/',\n",
       " '0003045xx/',\n",
       " '0003044xx/',\n",
       " '0003043xx/',\n",
       " '0003042xx/',\n",
       " '0003041xx/',\n",
       " '0003040xx/',\n",
       " '0003039xx/',\n",
       " '0003038xx/',\n",
       " '0003037xx/',\n",
       " '0003035xx/',\n",
       " '0003029xx/',\n",
       " '0003028xx/',\n",
       " '0003026xx/',\n",
       " '0003025xx/',\n",
       " '0003024xx/',\n",
       " '0003023xx/',\n",
       " '0003022xx/',\n",
       " '0003021xx/',\n",
       " '0003020xx/',\n",
       " '0003019xx/',\n",
       " '0003018xx/',\n",
       " '0003016xx/',\n",
       " '0003015xx/',\n",
       " '0003014xx/',\n",
       " '0003013xx/',\n",
       " '0003012xx/',\n",
       " '0003011xx/',\n",
       " '0003010xx/',\n",
       " '0003009xx/',\n",
       " '0003008xx/',\n",
       " '0003007xx/',\n",
       " '0003006xx/',\n",
       " '0003005xx/',\n",
       " '0003004xx/',\n",
       " '0003003xx/',\n",
       " '0003002xx/',\n",
       " '0003001xx/',\n",
       " '0003000xx/',\n",
       " '0002999xx/',\n",
       " '0002998xx/',\n",
       " '0002997xx/',\n",
       " '0002996xx/',\n",
       " '0002993xx/',\n",
       " '0002991xx/',\n",
       " '0002990xx/',\n",
       " '0002989xx/',\n",
       " '0002988xx/',\n",
       " '0002987xx/',\n",
       " '0002986xx/',\n",
       " '0002985xx/',\n",
       " '0002984xx/',\n",
       " '0002977xx/',\n",
       " '0002976xx/',\n",
       " '0002975xx/',\n",
       " '0002974xx/',\n",
       " '0002973xx/',\n",
       " '0002972xx/',\n",
       " '0002971xx/',\n",
       " '0002969xx/',\n",
       " '0002968xx/',\n",
       " '0002967xx/',\n",
       " '0002966xx/',\n",
       " '0002964xx/',\n",
       " '0002961xx/',\n",
       " '0002960xx/',\n",
       " '0002959xx/',\n",
       " '0002958xx/',\n",
       " '0002956xx/',\n",
       " '0002955xx/',\n",
       " '0002954xx/',\n",
       " '0002953xx/',\n",
       " '0002841xx/',\n",
       " '0002840xx/',\n",
       " '0002839xx/',\n",
       " '0002838xx/',\n",
       " '0002836xx/',\n",
       " '0002835xx/',\n",
       " '0002834xx/',\n",
       " '0002833xx/',\n",
       " '0002832xx/',\n",
       " '0002831xx/',\n",
       " '0002830xx/',\n",
       " '0002829xx/',\n",
       " '0002828xx/',\n",
       " '0002827xx/',\n",
       " '0002826xx/',\n",
       " '0002825xx/',\n",
       " '0002824xx/',\n",
       " '0002823xx/',\n",
       " '0002822xx/',\n",
       " '0002821xx/',\n",
       " '0002820xx/',\n",
       " '0002819xx/',\n",
       " '0002818xx/',\n",
       " '0002817xx/',\n",
       " '0002816xx/',\n",
       " '0002815xx/',\n",
       " '0002814xx/',\n",
       " '0002813xx/',\n",
       " '0002812xx/',\n",
       " '0002811xx/',\n",
       " '0002810xx/',\n",
       " '0002803xx/',\n",
       " '0002802xx/',\n",
       " '0002801xx/',\n",
       " '0002800xx/',\n",
       " '0002799xx/',\n",
       " '0002798xx/',\n",
       " '0002797xx/',\n",
       " '0002796xx/',\n",
       " '0002795xx/',\n",
       " '0002794xx/',\n",
       " '0002793xx/',\n",
       " '0002792xx/',\n",
       " '0002791xx/',\n",
       " '0002790xx/',\n",
       " '0002789xx/',\n",
       " '0002788xx/',\n",
       " '0002787xx/',\n",
       " '0002786xx/',\n",
       " '0002785xx/',\n",
       " '0002784xx/',\n",
       " '0002783xx/',\n",
       " '0002782xx/',\n",
       " '0002781xx/',\n",
       " '0002780xx/',\n",
       " '0002779xx/',\n",
       " '0002778xx/',\n",
       " '0002777xx/',\n",
       " '0002774xx/',\n",
       " '0002773xx/',\n",
       " '0002772xx/',\n",
       " '0002771xx/',\n",
       " '0002770xx/',\n",
       " '0002769xx/',\n",
       " '0002768xx/',\n",
       " '0002767xx/',\n",
       " '0002766xx/',\n",
       " '0002765xx/',\n",
       " '0002764xx/',\n",
       " '0002763xx/',\n",
       " '0002762xx/',\n",
       " '0002761xx/',\n",
       " '0002760xx/',\n",
       " '0002759xx/',\n",
       " '0002758xx/',\n",
       " '0002757xx/',\n",
       " '0002756xx/',\n",
       " '0002755xx/',\n",
       " '0002754xx/',\n",
       " '0002753xx/',\n",
       " '0002752xx/',\n",
       " '0002751xx/',\n",
       " '0002750xx/',\n",
       " '0002749xx/',\n",
       " '0002748xx/',\n",
       " '0002747xx/',\n",
       " '0002746xx/',\n",
       " '0002744xx/',\n",
       " '0002743xx/',\n",
       " '0002742xx/',\n",
       " '0002741xx/',\n",
       " '0002740xx/',\n",
       " '0002737xx/',\n",
       " '0002735xx/',\n",
       " '0002734xx/',\n",
       " '0002733xx/',\n",
       " '0002732xx/',\n",
       " '0002731xx/',\n",
       " '0002730xx/',\n",
       " '0002729xx/',\n",
       " '0002728xx/',\n",
       " '0002727xx/',\n",
       " '0002726xx/',\n",
       " '0002720xx/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For run directories without root files\n",
    "global_rundir_without_rootfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets use the JSON files to find the runs that we dont have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from: https://cms-service-dqm.web.cern.ch/cms-service-dqm/CAF/certification/\n",
      "list of relative urls:  ['Collisions18/13TeV/ReReco/Cert_314472-325175_13TeV_17SeptEarlyReReco2018ABC_PromptEraD_Collisions18_JSON.txt', 'Collisions17/13TeV/ReReco/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON_v1.txt', 'Collisions16/13TeV/ReReco/Final/Cert_271036-284044_13TeV_ReReco_07Aug2017_Collisions16_JSON.txt']\n"
     ]
    }
   ],
   "source": [
    "baseurl='https://cms-service-dqm.web.cern.ch/cms-service-dqm/CAF/certification/'\n",
    "json_runs_2018=\"Collisions18/13TeV/ReReco/Cert_314472-325175_13TeV_17SeptEarlyReReco2018ABC_PromptEraD_Collisions18_JSON.txt\"\n",
    "json_runs_2017=\"Collisions17/13TeV/ReReco/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON_v1.txt\"\n",
    "json_runs_2016=\"Collisions16/13TeV/ReReco/Final/Cert_271036-284044_13TeV_ReReco_07Aug2017_Collisions16_JSON.txt\"\n",
    "# json_runs_2015=\"Run2015/ZeroBias/\"\n",
    "\n",
    "\n",
    "json_runs=[json_runs_2018,json_runs_2017,json_runs_2016]#,url_runs_2015]\n",
    "print \"Fetching from: \"+baseurl\n",
    "print \"list of relative urls: \",json_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of runs in the json 478 in Collisions18/13TeV/ReReco/Cert_314472-325175_13TeV_17SeptEarlyReReco2018ABC_PromptEraD_Collisions18_JSON.txt\n",
      "315257 to 325172\n",
      "amount of runs in the json 474 in Collisions17/13TeV/ReReco/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON_v1.txt\n",
      "297050 to 306460\n",
      "amount of runs in the json 393 in Collisions16/13TeV/ReReco/Final/Cert_271036-284044_13TeV_ReReco_07Aug2017_Collisions16_JSON.txt\n",
      "273158 to 284044\n"
     ]
    }
   ],
   "source": [
    "json_runslist=[]\n",
    "for url in json_runs:\n",
    "    os.popen(\"curl -k -X GET \"+baseurl+url+\" > index.html\") \n",
    "\n",
    "\n",
    "\n",
    "    # os.popen(\"curl -k --cert proxy.cert --key proxy.cert -X GET \"+baseurl+url_runs_2017+\" > index.html\") \n",
    "    # This dumps the html page into a file which I called \"index.html\" \n",
    "    #Check the directory you are working in to see if it's there\n",
    "\n",
    "    f=codecs.open(\"index.html\",'rb')       # open the document\n",
    "    index = f.readlines()                  # read the document\n",
    "    a=str(index)                           # convert to string\n",
    "    soup = BeautifulSoup(a, 'html.parser') # create the soup object\n",
    "\n",
    "    a=re.findall(r\"(\\d{6})\",str(soup)) #find exactly 6 occurrences of a number\n",
    "                                    #in other words find 6 digit numbers\n",
    "\n",
    "    print \"amount of runs in the json\",len(a),\"in\",url\n",
    "    print a[0],\"to\",a[-1]\n",
    "    json_runslist.append(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding all missing runs for all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list = [item for sublist in json_runslist for item in sublist]\n",
    "len(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 good runs\n",
      "859 missing runs\n",
      "891 bad runs\n"
     ]
    }
   ],
   "source": [
    "good=[]\n",
    "missing=[]\n",
    "bad=[]\n",
    "for jrun in json_list:\n",
    "    for grun in global_runs_with_rootfiles:\n",
    "        if jrun==grun:\n",
    "            good.append(jrun)            \n",
    "    if jrun not in global_runs_with_rootfiles:\n",
    "        missing.append(jrun)\n",
    "        \n",
    "for grun in global_runs_with_rootfiles:\n",
    "    if grun not in json_list:\n",
    "        bad.append(grun)\n",
    "# print \"all good runs found\",good,\"\\n\"\n",
    "print len(good),\"good runs\"\n",
    "print len(missing),\"missing runs\"\n",
    "print len(bad),\"bad runs\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
